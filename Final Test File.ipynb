{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "pd.set_option('display.max_colwidth', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_model(X_train,y_train,X_test,y_test,featurez):\n",
    "    x_train_baseline = pd.DataFrame(x_train[featurez])\n",
    "    x_test_baseline = pd.DataFrame(x_test[featurez])\n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train_baseline, y_train)\n",
    "    y_pred = model.predict(x_test_baseline)\n",
    "    print(\"accuracy i : \"+ str(accuracy_score(y_pred, y_test)))\n",
    "    print(\"precision i : \"+ str(precision_score(y_pred, y_test)))\n",
    "    print(\"recall i : \"+ str(recall_score(y_pred, y_test)))\n",
    "    \n",
    "\n",
    "    fig,ax = plt.subplots(1,2, figsize = (5,5))    \n",
    "    \n",
    "    y_pred_prob = model.predict_proba(x_test_baseline)[:,1]\n",
    "    fpr, tpr, all_thresholds = roc_curve(y_test, y_pred_prob)\n",
    "#     hist(y_pred_prob)\n",
    "\n",
    "    ax[0].hist(y_pred_prob, bins = 20)\n",
    "    ax[1].plot(fpr,tpr)\n",
    "    \n",
    "    title('ROC Curve for Churn Prediction')\n",
    "    grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_model_loop(x_train,y_train,x_test,y_test,features_list):\n",
    "    for index, featurez in enumerate(features_list):\n",
    "        x_train_baseline = pd.DataFrame(x_train[featurez])\n",
    "        x_test_baseline = pd.DataFrame(x_test[featurez])\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators= 36, min_samples_split= 18, min_samples_leaf = 24, max_depth = 15)\n",
    "        model.fit(x_train_baseline, y_train)\n",
    "        y_pred = model.predict(x_test_baseline)\n",
    "        print(\"accuracy with model \" + str(index) + \":  \" + str(accuracy_score(y_pred, y_test)))\n",
    "        print(\"precision with model \" + str(index) + \":  \" +  str(precision_score(y_pred, y_test)))\n",
    "        print(\"recall with model \" + str(index) + \":  \" + str(recall_score(y_pred, y_test)))\n",
    "        print(' ')\n",
    "\n",
    "        y_pred_prob = model.predict_proba(x_test_baseline)[:,1]\n",
    "        fpr, tpr, all_thresholds = roc_curve(y_test, y_pred_prob)\n",
    "    \n",
    "        figure = figsize(10,10)\n",
    "        plot(fpr,tpr , label = 'model ' + str(index))\n",
    "        legend()\n",
    "\n",
    "        title('ROC Curve for Churn Prediction')\n",
    "        grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('data/churn_train.csv')\n",
    "df['last_trip_date'] = pd.to_datetime(df['last_trip_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df= pd.read_csv('data/churn_test.csv')\n",
    "test_df['last_trip_date'] = pd.to_datetime(test_df['last_trip_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LTV']=df['avg_surge']*df['trips_in_first_30_days']*df['avg_dist']\n",
    "df['miles']=df['trips_in_first_30_days']*df['avg_dist']\n",
    "df['avg_ride_price']=df['avg_surge']*df['avg_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['LTV']=test_df['avg_surge']*test_df['trips_in_first_30_days']*test_df['avg_dist']\n",
    "test_df['miles']=test_df['trips_in_first_30_days']*test_df['avg_dist']\n",
    "test_df['avg_ride_price']=test_df['avg_surge']*test_df['avg_dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = (df['last_trip_date'] < '2014-06-01') * 1\n",
    "df['constant'] = 1\n",
    "df['luxury_car_user'] = (df['luxury_car_user'] == 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['target'] = (test_df['last_trip_date'] < '2014-06-01') * 1\n",
    "test_df['constant'] = 1\n",
    "test_df['luxury_car_user'] = (test_df['luxury_car_user'] == 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_rating_of_driver_imputed'] = df['avg_rating_of_driver'].fillna(df.avg_rating_of_driver.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['avg_rating_of_driver_imputed'] = test_df['avg_rating_of_driver'].fillna(df.avg_rating_of_driver.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(pd.get_dummies(df['city']).join(df['target']).drop(['Winterfell','target'], 1))\n",
    "df = df.join(pd.get_dummies(df['phone'], drop_first = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.join(pd.get_dummies(test_df['city']).join(test_df['target']).drop(['Winterfell','target'], 1))\n",
    "test_df = test_df.join(pd.get_dummies(test_df['phone'], drop_first = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['avg_dist'] = df.avg_dist.transform(lambda x: 50 if x > 50 else x)\n",
    "# df['avg_ride_price'] = df.avg_ride_price.apply(lambda x: 50 if x > 50 else x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_dist_log'] = np.log(1 + df['avg_dist'])\n",
    "df['avg_ride_price_log'] = np.log(1 + df['avg_ride_price'])\n",
    "df['avg_surge_log'] = np.log(1 + df['avg_surge'])\n",
    "df['LTV_log'] = df['avg_surge_log']*df['trips_in_first_30_days']*df['avg_dist_log']\n",
    "df['surge_pct_log'] = np.log(1 + df['surge_pct'])\n",
    "df['weekday_pct_log'] = np.log(1 + df['weekday_pct'])\n",
    "df['miles_log'] = np.log(1 + df['miles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['avg_dist_log'] = np.log(1 + test_df['avg_dist'])\n",
    "test_df['avg_ride_price_log'] = np.log(1 + test_df['avg_ride_price'])\n",
    "test_df['avg_surge_log'] = np.log(1 + test_df['avg_surge'])\n",
    "test_df['LTV_log'] = test_df['avg_surge_log']*test_df['trips_in_first_30_days']*test_df['avg_dist_log']\n",
    "test_df['surge_pct_log'] = np.log(1 + test_df['surge_pct'])\n",
    "test_df['weekday_pct_log'] = np.log(1 + test_df['weekday_pct'])\n",
    "test_df['miles_log'] = np.log(1 + test_df['miles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test = train_test_split(df.drop('target',axis=1), df['target'], test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = ['constant','luxury_car_user']\n",
    "model1 = ['constant','luxury_car_user', 'trips_in_first_30_days']\n",
    "model2 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV']\n",
    "model3 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles']\n",
    "model4 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price']\n",
    "model5 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price', 'avg_rating_of_driver_imputed']\n",
    "\n",
    "model6 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing']\n",
    "model7 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone']\n",
    "model8 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log']\n",
    "model9 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log']\n",
    "model10 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'LTV', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log','surge_pct', 'weekday_pct']\n",
    "model11 = ['constant','luxury_car_user', 'LTV','trips_in_first_30_days', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log','surge_pct', 'weekday_pct', 'surge_pct_log']\n",
    "model12 = ['constant','luxury_car_user', 'LTV','trips_in_first_30_days', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log', 'weekday_pct', 'surge_pct_log', 'surge_pct']\n",
    "model13 = ['constant','luxury_car_user', 'LTV','trips_in_first_30_days', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log', 'weekday_pct', 'surge_pct_log', 'surge_pct', 'weekday_pct_log']\n",
    "model14 = ['constant','luxury_car_user', 'LTV','trips_in_first_30_days', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log', 'weekday_pct', 'surge_pct_log', 'surge_pct', 'weekday_pct_log', 'miles_log']\n",
    "\n",
    "model15 = ['constant','luxury_car_user', 'trips_in_first_30_days', 'weekday_pct', 'iPhone','surge_pct', \"King's Landing\", 'Astapor']\n",
    "model16 = ['luxury_car_user', 'LTV','trips_in_first_30_days', 'miles', 'avg_ride_price_log', 'avg_rating_of_driver_imputed','Astapor', 'King\\'s Landing', 'iPhone', 'LTV_log', 'weekday_pct', 'surge_pct_log', 'surge_pct', 'weekday_pct_log']\n",
    "\n",
    "features_list =[model0, model1, model2, model3, model4, model5, model6, model7, model8, model9, model10, model11, model12, model13, model14, model15, model16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target\n",
    "df.drop('target', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier(n_estimators= 36, min_samples_split= 18, min_samples_leaf = 24, max_depth = 15)\n",
    "# model2 = RandomForestClassifier(n_estimators= 29, min_samples_split = 32, min_samples_leaf = 6, max_depth =  14)\n",
    "\n",
    "model2.fit(df[model13], y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_df.target\n",
    "test_df.drop('target',1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(test_df[model13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model2.predict_proba(test_df[model13])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score 0.7933859675256965\n",
      "Recall score 0.8551701991008349\n",
      "Accuracy score 0.7711\n",
      "AUC score 0.8418174498093309\n",
      "Confusion Matrix\n",
      " [[2385 1387]\n",
      " [ 902 5326]]\n"
     ]
    }
   ],
   "source": [
    "print('Precision score', precision_score(test_y, y_pred))\n",
    "print('Recall score',recall_score(test_y, y_pred))\n",
    "print('Accuracy score',accuracy_score(test_y, y_pred))\n",
    "print('AUC score', roc_auc_score(test_y, y_pred_prob))\n",
    "print('Confusion Matrix\\n', confusion_matrix(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finished! Bada Boom!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-12ccf7d149ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogit_model_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "logit_model_loop(df,y_train,x_test,y_test,features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_loop(x_train,y_train,x_test,y_test,features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original - with outliers\n",
    "\n",
    "accuracy with model 0:  0.6355\n",
    "precision with model 0:  0.7119209718328661\n",
    "recall with model 0:  0.706451185587495\n",
    " \n",
    "accuracy with model 1:  0.67575\n",
    "precision with model 1:  0.9297824055533307\n",
    "recall with model 1:  0.6742497579864473\n",
    " \n",
    "accuracy with model 2:  0.6705833333333333\n",
    "precision with model 2:  0.9277800026698705\n",
    "recall with model 2:  0.6707199382358618\n",
    " \n",
    "accuracy with model 3:  0.67025\n",
    "precision with model 3:  0.9268455479909224\n",
    "recall with model 3:  0.6706916537867079\n",
    " \n",
    "accuracy with model 4:  0.6741666666666667\n",
    "precision with model 4:  0.9142971565879054\n",
    "recall with model 4:  0.6769793417020856\n",
    " \n",
    "accuracy with model 5:  0.6729166666666667\n",
    "precision with model 5:  0.9137631824856495\n",
    "recall with model 5:  0.6761161596207033\n",
    " \n",
    "accuracy with model 6:  0.7035833333333333\n",
    "precision with model 6:  0.8703777866773461\n",
    "recall with model 6:  0.7160114210410718\n",
    " \n",
    "accuracy with model 7:  0.71875\n",
    "precision with model 7:  0.8511547189961287\n",
    "recall with model 7:  0.7383047707271885\n",
    " \n",
    "accuracy with model 8:  0.7241666666666666\n",
    "precision with model 8:  0.8531571218795888\n",
    "recall with model 8:  0.7430531333565864\n",
    " \n",
    "accuracy with model 9:  0.72625\n",
    "precision with model 9:  0.8564944600186891\n",
    "recall with model 9:  0.7437978205425458\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changed - without outliers\n",
    "\n",
    "accuracy with model 0:  0.63275\n",
    "precision with model 0:  0.7090884682507719\n",
    "recall with model 0:  0.7022068598776922\n",
    " \n",
    "accuracy with model 1:  0.6715\n",
    "precision with model 1:  0.9276412941334408\n",
    "recall with model 1:  0.6700281198487346\n",
    " \n",
    "accuracy with model 2:  0.66925\n",
    "precision with model 2:  0.9292522486239764\n",
    "recall with model 2:  0.6678888460054033\n",
    " \n",
    "accuracy with model 3:  0.6703333333333333\n",
    "precision with model 3:  0.9304604644918781\n",
    "recall with model 3:  0.6684347574500916\n",
    " \n",
    "accuracy with model 4:  0.6731666666666667\n",
    "precision with model 4:  0.9182440596053162\n",
    "recall with model 4:  0.6736925046784201\n",
    " \n",
    "accuracy with model 5:  0.6725833333333333\n",
    "precision with model 5:  0.9152906430393342\n",
    "recall with model 5:  0.6739818109924871\n",
    " \n",
    "accuracy with model 6:  0.7021666666666667\n",
    "precision with model 6:  0.8687072090213451\n",
    "recall with model 6:  0.7136869968015882\n",
    " \n",
    "accuracy with model 7:  0.7175\n",
    "precision with model 7:  0.8552825882668814\n",
    "recall with model 7:  0.7337325809052171\n",
    " \n",
    "accuracy with model 8:  0.7200833333333333\n",
    "precision with model 8:  0.8582360048328634\n",
    "recall with model 8:  0.735165593376265\n",
    " \n",
    "accuracy with model 9:  0.7203333333333334\n",
    "precision with model 9:  0.8618606524365686\n",
    "recall with model 9:  0.7339659311764033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = RandomForestClassifier()\n",
    "# model.fit(x_train[model7], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'n_estimators': range(1, 50), 'min_samples_split': range(2, 50), 'min_samples_leaf': range(2, 50), 'max_depth': range(1, 20)},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators_range = range(1,50)\n",
    "min_samples_split_range = range(2,50)\n",
    "min_samples_leaf_range = range(2,50)\n",
    "max_depth_range = range(1,20)\n",
    "\n",
    "param_grid = dict({\n",
    "    'n_estimators': n_estimators_range,\n",
    "    'min_samples_split': min_samples_split_range,\n",
    "    'min_samples_leaf': min_samples_leaf_range,\n",
    "    'max_depth': max_depth_range,\n",
    "})\n",
    "\n",
    "search = RandomizedSearchCV(model, param_grid, cv = 10, scoring = 'accuracy')\n",
    "search.fit(x_train[model13], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7791785714285714\n",
      "{'n_estimators': 34, 'min_samples_split': 27, 'min_samples_leaf': 12, 'max_depth': 11}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=11, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=12, min_samples_split=27,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=34, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.03883705, 0.06375214, 0.14607049, 0.14499842,\n",
       "       0.3744322 , 0.1021648 , 0.0186103 , 0.06519148, 0.04594313])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-0bd6af871dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model2 = RandomForestClassifier(n_estimators= 29, min_samples_split = 32, min_samples_leaf = 6, max_depth =  14)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "model2 = RandomForestClassifier(n_estimators= 36, min_samples_split= 18, min_samples_leaf = 24, max_depth = 15)\n",
    "# model2 = RandomForestClassifier(n_estimators= 29, min_samples_split = 32, min_samples_leaf = 6, max_depth =  14)\n",
    "\n",
    "model2.fit(x_train[model13], y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(x_test[model13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score 0.7965647460104763\n",
      "Recall score 0.8731472826812658\n",
      "Accuracy score 0.7816666666666666\n",
      "Confusion Matrix\n",
      " [[2841 1670]\n",
      " [ 950 6539]]\n"
     ]
    }
   ],
   "source": [
    "print('Precision score', precision_score(y_test, y_pred))\n",
    "print('Recall score',recall_score(y_test, y_pred))\n",
    "print('Accuracy score',accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost = AdaBoostClassifier()\n",
    "adaboost.fit(x_train[model13], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'n_estimators': range(1, 50), 'learning_rate': [0.0, 0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_estimators_range = range(1,50)\n",
    "learning_rate_range = list(np.arange(0,1,0.05))\n",
    "\n",
    "param_grid = dict({\n",
    "    'n_estimators': n_estimators_range,\n",
    "    'learning_rate': learning_rate_range,\n",
    "})\n",
    "\n",
    "search = RandomizedSearchCV(adaboost, param_grid, cv = 10, scoring = 'accuracy')\n",
    "search.fit(x_train[model13], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier()\n",
    "GB.fit(df[model13], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'n_estimators': range(1, 50), 'learning_rate': [0.0, 0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators_range = range(1,50)\n",
    "learning_rate_range = list(np.arange(0,1,0.05))\n",
    "\n",
    "param_grid = dict({\n",
    "    'n_estimators': n_estimators_range,\n",
    "    'learning_rate': learning_rate_range,\n",
    "})\n",
    "\n",
    "search = RandomizedSearchCV(GB, param_grid, cv = 10, scoring = 'accuracy')\n",
    "search.fit(df[model13], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783475\n",
      "{'n_estimators': 22, 'learning_rate': 0.8}\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.8, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=22,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(search.best_score_)\n",
    "print(search.best_params_)\n",
    "print(search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.8, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=22,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GB = GradientBoostingClassifier(n_estimators= 22, learning_rate = 0.8)\n",
    "GB.fit(df[model13], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = GB.predict(test_df[model13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score 0.793849828332587\n",
      "Recall score 0.8538856775850996\n",
      "Accuracy score 0.7709\n",
      "Confusion Matrix\n",
      " [[2391 1381]\n",
      " [ 910 5318]]\n"
     ]
    }
   ],
   "source": [
    "print('Precision score', precision_score(test_y, y_pred))\n",
    "print('Recall score',recall_score(test_y, y_pred))\n",
    "print('Accuracy score',accuracy_score(test_y, y_pred))\n",
    "print('Confusion Matrix\\n', confusion_matrix(test_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7725\n",
      "{'n_estimators': 42, 'learning_rate': 0.65}\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.65, n_estimators=42, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "# print(search.best_score_)\n",
    "# print(search.best_params_)\n",
    "# print(search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Precision score 0.7965647460104763\n",
    "# Recall score 0.8731472826812658\n",
    "# Accuracy score 0.7816666666666666\n",
    "# Confusion Matrix\n",
    "#  [[2841 1670]\n",
    "#  [ 950 6539]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
